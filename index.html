<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Detection AI</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            margin: 20px;
        }
        video, canvas {
            border: 1px solid #ccc;
            margin: 10px;
        }
    </style>
</head>
<body>
    <h1>Live Face Tracker - Find Person in Crowd</h1>
    <video id="video" width="640" height="480" autoplay muted></video>
    <canvas id="canvas" width="640" height="480"></canvas>
    <br>
    <button id="capture">Capture Reference Face</button>
    <p>Click to capture the face you want to find in the crowd.</p>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        let referenceDescriptor = null;

        async function loadModels() {
            await faceapi.loadSsdMobilenetv1Model('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights/');
            await faceapi.loadFaceLandmarkModel('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights/');
            await faceapi.loadFaceRecognitionModel('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights/');
        }

        async function startVideo() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                video.onloadedmetadata = () => {
                    video.play();
                    detectFaces();
                };
            } catch (err) {
                console.error('Error accessing camera:', err);
                alert('Camera access denied or not available.');
            }
        }

        document.getElementById('capture').addEventListener('click', async () => {
            const detection = await faceapi.detectSingleFace(video).withFaceLandmarks().withFaceDescriptor();
            if (detection) {
                referenceDescriptor = detection.descriptor;
                alert('Reference face captured! Now tracking in crowd.');
            } else {
                alert('No face detected. Please position your face clearly.');
            }
        });

        async function detectFaces() {
            const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors();
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.drawImage(video, 0, 0);
            detections.forEach(detection => {
                const box = detection.detection.box;
                let color = 'red';
                if (referenceDescriptor) {
                    const distance = faceapi.euclideanDistance(detection.descriptor, referenceDescriptor);
                    if (distance < 0.6) {
                        color = 'green'; // Match
                    }
                }
                ctx.strokeStyle = color;
                ctx.lineWidth = 3;
                ctx.strokeRect(box.x, box.y, box.width, box.height);
            });
            requestAnimationFrame(detectFaces);
        }

        loadModels().then(() => {
            startVideo();
        });
    </script>
</body>
</html>